{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text-classification.ipynb","provenance":[],"collapsed_sections":["iL5tqbo32Oww"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyMtpid+Fj5DvkLxmzTF7VbW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zSClP_dUe0Ly","colab_type":"text"},"source":["# Text classification task"]},{"cell_type":"code","metadata":{"id":"a9qgE3CMnuwS","colab_type":"code","colab":{}},"source":["#----- mount colab env (Necessary for Colab use)\n","# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJ6DL7Pmn8_j","colab_type":"text"},"source":["## Packages"]},{"cell_type":"code","metadata":{"id":"vzfQvs7foGEO","colab_type":"code","colab":{}},"source":["#%tensorflow_version 1.x\n","\n","#---- magic trio + special guest\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#---- utils\n","import pickle\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils import to_categorical\n","import keras.backend as K\n","import gensim\n","from gensim.models import Word2Vec\n","from sklearn.ensemble import RandomForestClassifier\n","from tqdm.autonotebook import tqdm\n","from collections import defaultdict\n","import logging\n","from keras.models import clone_model\n","from sklearn import utils\n","\n","# if you want to try the concat version of doc2vec you need to install this dependency\n","#!pip install -q testfixtures\n","#from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n","\n","import time\n","\n","#---- dl\n","from keras.layers import Input, Dense, Dropout, LeakyReLU\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.layers import ReLU\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from keras.regularizers import l1_l2, l1, l2\n","from keras.optimizers import RMSprop, Nadam, SGD\n","\n","#!pip install -q livelossplot\n","\n","from livelossplot.keras import PlotLossesCallback"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bOo8oxnafVTo","colab_type":"code","colab":{}},"source":["with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/df_preprocessed_eng.pckle\", \"rb\") as infile:\n","  data = pickle.load(infile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Lhd8xR7C-A_","colab_type":"code","colab":{}},"source":["#----- remapping categories\n","\n","category_remap_dict = {\n","    \"LGBT\": \"socializing\",\n","    \"singles\": \"socializing\",\n","    \"fashion/beauty\": \"health/wellbeing\",\n","    \"movies/film\": \"socializing\",\n","    \"book clubs\": \"education/learning\",\n","    \"sci-fi/fantasy\": \"games\",\n","    \"support\": \"health/wellbeing\",\n","    \"cars/motorcycles\": \"outdoors/adventure\"\n","}\n","\n","\n","data[\"remap_category\"] = data.category.map(lambda x: category_remap_dict[x] if x in category_remap_dict.keys() else x )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hZZCE9ZeZj0J","colab_type":"text"},"source":["## Inspection"]},{"cell_type":"code","metadata":{"id":"Kv7jaNahheIn","colab_type":"code","colab":{}},"source":["data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pc1qrxKVIHel","colab_type":"code","colab":{}},"source":["plt.figure(figsize = (15,20))\n","sns.set_style(\"whitegrid\")\n","sns.countplot(y = data.remap_category)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Se6jq31sMUg4","colab_type":"text"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"mQIRl3FKG4qo","colab_type":"code","colab":{}},"source":["#----- k-folds cross validation\n","def load_data_kfold(k, X_train, y_train):\n","    \n","    folds = list(StratifiedKFold(n_splits = k, shuffle = True,\n","                                 random_state = 42).split(X_train, y_train))\n","    \n","    return folds\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3fm5nptowrh","colab_type":"code","colab":{}},"source":["#----- top-k accuracy score\n","def top_k_acc(model, X_test, y_true, k_top = 3, ml = False):\n","  if ml:\n","    probs = model.predict_proba(X_test)\n","  else:  \n","    probs = model.predict(X_test)\n","  #y_true = np.argmax(y_true, axis = 1)\n","  topn = np.argsort(probs, axis = 1)[:,-k_top:]\n","  return np.mean(np.array([1 if y_true[k_top] in topn[k_top] else 0 for k_top in range(len(topn))]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pFQ9LY9sMZ-_","colab_type":"code","colab":{}},"source":["#----- averaging for features (word2vec pre-trained)\n","def word_averaging(wv, words):\n","    all_words, mean = set(), []\n","    \n","    for word in words:\n","        if isinstance(word, np.ndarray):\n","            mean.append(word)\n","        elif word in wv.vocab:\n","            mean.append(wv.syn0norm[wv.vocab[word].index])\n","            all_words.add(wv.vocab[word].index)\n","\n","    if not mean:\n","        logging.warning(\"cannot compute similarity with no input %s\", words)\n","        # FIXME: remove these examples in pre-processing\n","        return np.zeros(wv.vector_size,)\n","\n","    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n","    return mean"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PThwCszaMnTt","colab_type":"code","colab":{}},"source":["#---- list averaging (word2vec pre-trained)\n","def  word_averaging_list(wv, text_list):\n","    return np.vstack([word_averaging(wv, post) for post in text_list ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7jG8FUPAMwZ-","colab_type":"code","colab":{}},"source":["#----- Embedding vectorizer (Tf-idf)\n","class TfidfEmbeddingVectorizer(object):\n","      def __init__(self, word2vec):\n","          self.word2vec = word2vec\n","          self.word2weight = None\n","          self.dim = len(next(iter(word2vec.values())))\n","\n","      def fit(self, X, y):\n","          tfidf = TfidfVectorizer(analyzer=lambda x: x)\n","          tfidf.fit(X)\n","          # if a word was never seen - it must be at least as infrequent\n","          # as any of the known words - so the default idf is the max of \n","          # known idf's\n","          max_idf = max(tfidf.idf_)\n","          self.word2weight = defaultdict(\n","              lambda: max_idf,\n","              [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n","\n","          return self\n","\n","      def transform(self, X):\n","          return np.array([\n","                  np.mean([self.word2vec[w] * self.word2weight[w]\n","                          for w in words if w in self.word2vec] or\n","                          [np.zeros(self.dim)], axis=0)\n","                  for words in X\n","              ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WoYXOYILNFQA","colab_type":"code","colab":{}},"source":["#----- Embedding vectorizer (mean)\n","class MeanEmbeddingVectorizer(object):\n","      def __init__(self, word2vec):\n","          self.word2vec = word2vec\n","          # if a text is empty we should return a vector of zeros\n","          # with the same dimensionality as all the other vectors\n","          self.dim = len(next(iter(word2vec.values())))\n","\n","      def fit(self, X, y):\n","          return self\n","\n","      def transform(self, X):\n","          return np.array([\n","              np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n","                      or [np.zeros(self.dim)], axis=0)\n","              for words in X\n","          ])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-sVzov9KZeXK","colab_type":"text"},"source":["# Text representation"]},{"cell_type":"markdown","metadata":{"id":"iL5tqbo32Oww","colab_type":"text"},"source":["## Pre-trained word2vec model (Deprecated, performanes were similar to not pre-trained)"]},{"cell_type":"code","metadata":{"id":"VStJhlH1qq7R","colab_type":"code","colab":{}},"source":["# wv = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/Colab Notebooks/TM&S/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n","# wv.init_sims(replace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUybHaN8j0Ql","colab_type":"code","colab":{}},"source":["#w2v = dict(zip(wv.wv.index2word, wv.wv.syn0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMJ8b1mfkuNZ","colab_type":"code","colab":{}},"source":["#vect = TfidfEmbeddingVectorizer(w2v)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JX48V98_k7qr","colab_type":"code","colab":{}},"source":["#vect.fit(X, data.remap_category)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NalmJVelPEW","colab_type":"code","colab":{}},"source":["#train_vect = vect.transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouPRGT8xrAwJ","colab_type":"code","colab":{}},"source":["#X_word_average = word_averaging_list(wv, data.desc_lemm_no_badwords)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wgOQpROR2XIK","colab_type":"text"},"source":["## Text-representation function"]},{"cell_type":"code","metadata":{"id":"JhZ5WagpizFy","colab_type":"code","colab":{}},"source":["#----- text-representation function\n","def text_represent(method, X, y, df_token = None, size = 300):\n","  '''\n","  @params\n","  method = str, one of ['bow-tfidf', 'bow-count', 'w2v-tfidf', 'w2v-mean']\n","         bow-tfidf: bag-of-words representation using Tf-Idf\n","         bow-count: bag-of-words representation using token count\n","         w2v-tfidf: word-embedding representation using word2vec and Tf-Idf\n","         w2v-mean:  word-embedding representation using word2vec and mean \n","  X = list of lists, list containing a list of token for each document\n","  df_token = pd.Series, a pandas series containing the token token for every document (To use with bow methods)\n","  y = pd.Series, a pandas series containing the labels of every document\n","  '''\n","  #----- bag-of-words tf-idf\n","  if method == 'bow-tfidf':\n","    print(\"Creating bag-of-words tf-idf representation...\")\n","    tfidf = TfidfVectorizer(max_features = size, analyzer = 'word', ngram_range = (1,1), sublinear_tf = True)\n","    train_vect = tfidf.fit_transform(df_token)\n","\n","  #----- bag-of-words count vector\n","  elif method == 'bow-count':\n","    print(\"Creating bag-of-words count representation...\")\n","    count_vect = CountVectorizer(max_features = size, analyzer = 'word', ngram_range = (1,1))\n","    train_vect = count_vect.fit_transform(df_token)\n","\n","  #----- word embedding word2vec tf-idf\n","  elif method == 'w2v-tfidf':\n","    print(\"Creating word embedding representation using word2vec & tf-idf...\")\n","    print(\"It may took a while\")\n","    model = gensim.models.Word2Vec(X, size = size, workers = 4, iter = 10)\n","    w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n","    vect = TfidfEmbeddingVectorizer(w2v)\n","    vect.fit(X, y)\n","    train_vect = vect.transform(X)\n","\n","  #----- word embedding word2vec mean\n","  elif method == 'w2v-mean':\n","    print(\"Creating word embedding representation using word2vec and mean...\")\n","    print(\"It may took a while\")\n","    model = gensim.models.Word2Vec(X, size = size, workers = 4, iter = 10)\n","    w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n","    vect = MeanEmbeddingVectorizer(w2v)\n","    vect.fit(X, y)\n","    train_vect = vect.transform(X)\n","  \n","  #----- raise Error\n","  else:\n","    print(\"No valid text-representation method selected\")\n","    raise ValueError\n","\n","  print(\"done\")\n","  return train_vect\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CKoRO8OSIXBG","colab_type":"text"},"source":["## Create dataframes for evaluations and scoring"]},{"cell_type":"code","metadata":{"id":"dBnHbjpVGpKU","colab_type":"code","colab":{}},"source":["#----- evaluations dataframe\n","# columns = pd.MultiIndex.from_product([['Stemming', 'Stemming+Badwords', 'Lemmatization', 'Lemmatization+Badwords'], ['Acc', 'Macro F-Measure', 'Weighted F-Measure']],\n","#                                      names = ['Processing method', 'Feature Extraction Method'])\n","# #columns = ['model', 'representation', 'f1-macro', 'f1-weighted', 'accuracy']\n","# evaluations = pd.DataFrame(columns = columns)\n","# evaluations = evaluations.append(pd.DataFrame.from_dict({'Stemming':{'Acc': 0.692, 'Weighted F-Measure':0.687, 'Macro F-Measure':0.659},\n","#                                                          'Stemming+Badwords':{'Acc': 0.692, 'Weighted F-Measure':0.687, 'Macro F-Measure':0.659},\n","#                                                          'Lemmatization':{'Acc': 0.692, 'Weighted F-Measure':0.687, 'Macro F-Measure':0.661},\n","#                                                          'Lemmatization+Badwords':{'Acc': 0.693, 'Weighted F-Measure':0.688, 'Macro F-Measure':0.662}}).unstack().rename('Count'))\n","# evaluations = evaluations.append(pd.DataFrame.from_dict({'Stemming':{'Acc': 0.689, 'Weighted F-Measure':0.685, 'Macro F-Measure':0.656},\n","#                                                          'Stemming+Badwords':{'Acc': 0.691, 'Weighted F-Measure':0.686, 'Macro F-Measure':0.660},\n","#                                                          'Lemmatization':{'Acc': 0.688, 'Weighted F-Measure':0.684, 'Macro F-Measure':0.656},\n","#                                                          'Lemmatization+Badwords':{'Acc': 0.688, 'Weighted F-Measure':0.684, 'Macro F-Measure':0.658}}).unstack().rename('Tf-idf'))\n","# evaluations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAHyXoifPGLB","colab_type":"code","colab":{}},"source":["#----- dump evaluations dataframe\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/evaluations.pckl\", \"wb\") as outfile:\n","#   pickle.dump(evaluations, outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"65zKT7gvIovn","colab_type":"code","colab":{}},"source":["#----- create scoring dataframe\n","# columns1 = pd.MultiIndex.from_product([[\"\"],['Model', 'Processing method','Feature Extraction Method']])\n","# columns2 = pd.MultiIndex.from_product([['Acc', 'Top-3 Acc', 'Macro F-Measure', 'Weighted F-Measure'],\n","#                                                                   ['value', 'std']])\n","\n","# scores = pd.concat([pd.DataFrame(columns = columns1), pd.DataFrame(columns = columns2)], axis = 1)\n","#scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbRjb70IIwof","colab_type":"code","colab":{}},"source":["#----- dump scoring dataframe\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"wb\") as outfile:\n","#   pickle.dump(scores, outfile)\n","#   outfile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZCZPQw_yczO","colab_type":"text"},"source":["## Doc2Vec section (choose to use DBOW, DM or concatenate the two models)"]},{"cell_type":"code","metadata":{"id":"R8L4gIOHRNlZ","colab_type":"code","colab":{}},"source":["#----- tag all the documents\n","tagged = data.apply(lambda r: gensim.models.doc2vec.TaggedDocument(words=str(r.desc_stemm).split(\" \"), tags=[r.remap_category]), axis=1)\n","cores = 4\n","\n","#----- dbow model\n","model_dbow = gensim.models.Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores)\n","model_dbow.build_vocab([x for x in tqdm(tagged.values)]) \n","\n","#----- dmm model\n","#model_dmm = gensim.models.Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=10, negative=5, min_count=1, workers=cores, alpha=0.065, min_alpha=0.065)\n","#model_dmm.build_vocab([x for x in tqdm(tagged.values)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NxZzoPnRSyE-","colab_type":"code","colab":{}},"source":["#---- train dbow model\n","%%time\n","for epoch in range(10):\n","    model_dbow.train(utils.shuffle([x for x in tqdm(tagged.values)]), total_examples=len(tagged.values), epochs=1)\n","    model_dbow.alpha -= 0.002\n","    model_dbow.min_alpha = model_dbow.alpha"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyH2nNj4L-n8","colab_type":"code","colab":{}},"source":["#----- train dmm model\n","# %%time\n","# for epoch in range(10):\n","#     model_dmm.train(utils.shuffle([x for x in tqdm(tagged.values)]), total_examples=len(tagged.values), epochs=1)\n","#     model_dmm.alpha -= 0.002\n","#     model_dmm.min_alpha = model_dmm.alpha"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUyybPNZMSOi","colab_type":"code","colab":{}},"source":["#----- make the models light\n","# model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n","# model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NahnZf0qMS37","colab_type":"code","colab":{}},"source":["#----- concatenate models\n","#new_model = ConcatenatedDoc2Vec([model_dbow, model_dmm])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4C6vDqLzTcKq","colab_type":"code","colab":{}},"source":["def vec_for_learning(model, tagged_docs):\n","    #sents = tagged_docs.values\n","    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in tqdm(tagged_docs.values)])\n","    return targets, regressors"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBfmVDrMThmo","colab_type":"code","colab":{}},"source":["#----- feature extraction\n","y, X = vec_for_learning(model_dbow, tagged)\n","train_vect = np.asarray(X)\n","y_ar = np.asarray(y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SZbD2F_3ieK","colab_type":"code","colab":{}},"source":["#----- save doc2vec model\n","#np.savez_compressed(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/d2v_stemm_concat.npz\", X = train_vect, y = y_ar)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"buejylmzI2Mq","colab_type":"text"},"source":["## Arrange data"]},{"cell_type":"code","metadata":{"id":"XRTrpSmhD0se","colab_type":"code","colab":{}},"source":["#----- create list of lists token representation\n","X = []\n","for i, j in tqdm(data.iterrows()):\n","  X.append(j.desc_lemm_no_badwords.split())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FkBGc402I5NB","colab_type":"code","colab":{}},"source":["#%%time\n","#----- text representation (choose representation and the preprocessing phase)\n","train_vect = text_represent(method = 'w2v-mean', X = X, df_token = data.desc_lemm_no_badwords, y = data.remap_category)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgcRkFpyCd60","colab_type":"code","colab":{}},"source":["#----- eventually save the representation\n","#np.savez_compressed(\"/content/drive/My Drive/Colab Notebooks/TM&S/w2v_mean_lemm_no_badwords.npz\", X = train_vect)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUm3AMuSJqoY","colab_type":"code","colab":{}},"source":["#----- if saved, load the representation\n","# with np.load(\"/content/drive/My Drive/Colab Notebooks/TM&S/w2v_tfidf.npz\") as infile:\n","#   train_vect = infile['X']\n","#   #category = infile['y']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gy1XGM0dj3jT","colab_type":"code","colab":{}},"source":["#----- handling labels\n","y = data.remap_category\n","lb = LabelEncoder()\n","y = lb.fit_transform(y)\n","y = to_categorical(y, num_classes = 24)\n","num_labels = y.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZvRwg-c_Jfe9","colab_type":"text"},"source":["# Models"]},{"cell_type":"markdown","metadata":{"id":"f2vpp-HVZaFb","colab_type":"text"},"source":["## Neural Network"]},{"cell_type":"markdown","metadata":{"id":"07aWHLRiKqH3","colab_type":"text"},"source":["### K-folds cross validation scoring"]},{"cell_type":"code","metadata":{"id":"8yRuRSUVl9r0","colab_type":"code","colab":{}},"source":["#----- if saved, load\n","# with np.load(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/d2v_lemm_no_badwords.npz\") as infile:\n","#   train_vect = infile['X']\n","#   infile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7F5z6R2ik9yl","colab_type":"code","colab":{}},"source":["#----- nn model\n","K.clear_session()\n","\n","model = Sequential()\n","model.add(Dense(1024, input_dim = train_vect.shape[1]))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(512))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(128))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(24, activation=\"softmax\"))\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2rf-qT6scQn","colab_type":"code","colab":{}},"source":["#----- cross-validation cicle\n","k = 5\n","acc = []\n","top_3 = []\n","f1_macro = []\n","f1_weighted = []\n","\n","lb = LabelEncoder()\n","lb.fit(data.remap_category)\n","\n","\n","folds = load_data_kfold(k, train_vect, data.remap_category.values)\n","for j, (train_idx, test_idx) in enumerate(tqdm(folds)):\n","  es = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True)\n","  rlrop = ReduceLROnPlateau(monitor='val_loss', patience = 5, factor = .5, min_lr = 1e-6)\n","\n","  X_train_cv = train_vect[train_idx]\n","  y_train_cv = data.remap_category.values[train_idx]\n","  X_test_cv = train_vect[test_idx]\n","  y_test_cv = data.remap_category.values[test_idx]\n","\n","  y_train_cv = lb.transform(y_train_cv)\n","  y_test_cv = lb.transform(y_test_cv)\n","  y_train_cv = to_categorical(y_train_cv, num_classes = 24)\n","  y_test_cv = to_categorical(y_test_cv, num_classes = 24)\n","\n","\n","  #----- creation of validation set for EarlyStopping\n","  X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_cv, y_train_cv, test_size = .1,\n","                                                      random_state = 42,\n","                                                      stratify = y_train_cv)\n","  kfold_model = clone_model(model)\n","  kfold_model.compile(optimizer = Adam(), loss = \"categorical_crossentropy\", metrics = ['acc'])\n","\n","  kfold_model.fit(X_train_cv, y_train_cv,\n","                  batch_size = 2048,\n","                  epochs = 100,\n","                  verbose = 0,\n","                  validation_data = (X_val, y_val),\n","                  callbacks = [es, rlrop],\n","                  use_multiprocessing = True)\n","\n","#----- evaluation\n","  y_pred = kfold_model.predict_classes(X_test_cv)\n","  y_true = np.argmax(y_test_cv, axis = 1)\n","  f1_macro.append(f1_score(y_true, y_pred, average = 'macro'))\n","  f1_weighted.append(f1_score(y_true, y_pred, average = 'weighted'))\n","  acc.append(accuracy_score(y_true, y_pred))\n","  top_3.append(top_k_acc(kfold_model, X_test_cv, y_true, k_top = 3))\n","\n","\n","print(\"Acc: {} (+/- {})\\nTop-3: {} (+/- {})\\nF1-Macro: {} (+/- {})\\nF1-Weighted: {} (+/- {})\".format(round(np.mean(acc), 3), round(np.std(acc), 3),\n","                              round(np.mean(top_3), 3), round(np.std(top_3), 3),\n","                              round(np.mean(f1_macro), 3), round(np.std(f1_macro), 3),\n","                              round(np.mean(f1_weighted), 3), round(np.std(f1_weighted), 3)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWkjrYGAwidh","colab_type":"code","colab":{}},"source":["#----- save results\n","\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"rb\") as infile:\n","#   scores = pickle.load(infile)\n","#   infile.close()\n","\n","# #scores\n","# #['stem+badwords', 'stem+badwords', 'lemm+badwords']\n","# processing_method = \"Lemmatization+Badwords removal\"\n","# #['w2v tf-idf', 'w2v mean', 'doc2vec']\n","# representation = 'Doc2Vec'\n","# #[0,1,2]\n","# row = 2\n","\n","# scores.loc[row, (\"\", \"Model\")] = \"NN\"\n","# scores.loc[row, (\"\", \"Processing method\")] = processing_method\n","# scores.loc[row, (\"\", \"Feature Extraction Method\")] = representation\n","# scores.loc[row, (\"Acc\", \"value\")] = round(np.mean(acc), 3)\n","# scores.loc[row, (\"Acc\", \"std\")] = round(np.std(acc), 3)\n","# scores.loc[row, (\"Top-3 Acc\", \"value\")] = round(np.mean(top_3), 3)\n","# scores.loc[row, (\"Top-3 Acc\", \"std\")] = round(np.std(top_3), 3)\n","# scores.loc[row, (\"Macro F-Measure\", \"value\")] = round(np.mean(f1_macro), 3)\n","# scores.loc[row, (\"Macro F-Measure\", \"std\")] = round(np.std(f1_macro), 3)\n","# scores.loc[row, (\"Weighted F-Measure\", \"value\")] = round(np.mean(f1_weighted), 3)\n","# scores.loc[row, (\"Weighted F-Measure\", \"std\")] = round(np.std(f1_weighted), 3)\n","\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"wb\") as outfile:\n","#   pickle.dump(scores, outfile)\n","#   outfile.close()\n","\n","\n","#scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqHu7i-7HBfQ","colab_type":"text"},"source":["### Punctual evaluation"]},{"cell_type":"code","metadata":{"id":"bpatQRge9PI2","colab_type":"code","colab":{}},"source":["#----- punctual evaluations\n","\n","#----- nn model\n","K.clear_session()\n","\n","model = Sequential()\n","model.add(Dense(1024, input_dim = train_vect.shape[1]))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(512))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(128))\n","model.add(ReLU())\n","model.add(Dropout(.5))\n","model.add(Dense(24, activation=\"softmax\"))\n","\n","# model.summary()\n","\n","#----- train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(train_vect, y, test_size = .1, random_state = 1, stratify = y)\n","num_labels = y.shape[1]\n","\n","es = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights=True)\n","rlrop = ReduceLROnPlateau(monitor='val_loss', patience = 5, factor = .5, min_lr = 1e-6)\n","model.fit(X_train, y_train,\n","          batch_size = 2048,\n","          epochs = 100,\n","          verbose = 0,\n","          validation_split = .1,\n","          callbacks = [PlotLossesCallback(), es, rlrop],\n","          use_multiprocessing = True)\n","\n","y_pred = model.predict_classes(X_test)\n","y_true = np.argmax(y_test, axis = 1)\n","f1_macro = f1-score(y_true, y_pred, average = 'macro')\n","f1_weighted = f1-score(y_true, y_pred, average = 'weighted')\n","acc = accuracy_score(y_true, y_pred)\n","\n","acc, f1_macro, f1_weighted\n","\n","# model = 'W2V Tf-idf'\n","# representation = 'Stemming'\n","\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/evaluations.pckl\", \"rb\") as infile:\n","#   evaluations = pickle.load(infile)\n","\n","# evaluations.loc[model, (representation, 'Acc')] = acc\n","# evaluations.loc[model, (representation, 'Macro F-Measure')] = f1_macro\n","# evaluations.loc[model, (representation, 'Weighted F-Measure')] = f1_weighted\n","\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/evaluations.pckl\", \"wb\") as outfile:\n","#   pickle.load(evaluations, outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"deiLIBQ-ZUtX","colab_type":"text"},"source":["## Random Forest"]},{"cell_type":"markdown","metadata":{"id":"mwdf_rB5J8Dw","colab_type":"text"},"source":["### K-folds cross validation scoring"]},{"cell_type":"code","metadata":{"id":"sLRmUvXNBIHZ","colab_type":"code","colab":{}},"source":["#----- load if saved\n","#train_vect = np.load(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/d2v_lemm_no_badwords.npz\")['X']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mU5iAzx10Rt0","colab_type":"code","colab":{}},"source":["#----- cross-validation cicle\n","k = 5\n","acc = []\n","top_3 = []\n","f1_macro = []\n","f1_weighted = []\n","lb = LabelEncoder()\n","lb.fit(data.remap_category)\n","\n","folds = load_data_kfold(k, train_vect, data.remap_category.values)\n","for j, (train_idx, test_idx) in enumerate(tqdm(folds)):\n","\n","  X_train_cv = train_vect[train_idx]\n","  y_train_cv = data.remap_category.values[train_idx]\n","  X_test_cv = train_vect[test_idx]\n","  y_test_cv = data.remap_category.values[test_idx]\n","\n","  rf = RandomForestClassifier(n_jobs=-1)\n","  rf = rf.fit(X_train_cv, y_train_cv)\n","\n","\n","\n","\n","#----- evaluation\n","  y_pred = rf.predict(X_test_cv)\n","  #y_true = np.argmax(y_test_cv, axis = 1)\n","  f1_macro.append(f1_score(y_test_cv, y_pred, average = 'macro'))\n","  f1_weighted.append(f1_score(y_test_cv, y_pred, average = 'weighted'))\n","  acc.append(accuracy_score(y_test_cv, y_pred))\n","  top_3.append(top_k_acc(rf, X_test_cv, lb.transform(y_test_cv), k_top = 3, ml = True))\n","\n","\n","print(\"Acc: {} (+/- {})\\nTop-3: {} (+/- {})\\nF1-Macro: {} (+/- {})\\nF1-Weighted: {} (+/- {})\".format(round(np.mean(acc), 3), round(np.std(acc), 3),\n","                              round(np.mean(top_3), 3), round(np.std(top_3), 3),\n","                              round(np.mean(f1_macro), 3), round(np.std(f1_macro), 3),\n","                              round(np.mean(f1_weighted), 3), round(np.std(f1_weighted), 3)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qkrh-CpCCfYX","colab_type":"code","colab":{}},"source":["#----- save results\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"rb\") as infile:\n","#   scores = pickle.load(infile)\n","#   infile.close()\n","\n","# #scores\n","# #['Stemming+Badwords removal', 'Stemming+Badwords removal', 'Lemmatization+Badwords removal']\n","# processing_method = \"Lemmatization+Badwords removal\"\n","# #['W2V Tf-idf', 'W2V Mean', 'Doc2Vec']\n","# representation = 'Doc2Vec'\n","# #[3,4,5]\n","# row = 5\n","\n","# scores.loc[row, (\"\", \"Model\")] = \"RF\"\n","# scores.loc[row, (\"\", \"Processing method\")] = processing_method\n","# scores.loc[row, (\"\", \"Feature Extraction Method\")] = representation\n","# scores.loc[row, (\"Acc\", \"value\")] = round(np.mean(acc), 3)\n","# scores.loc[row, (\"Acc\", \"std\")] = round(np.std(acc), 3)\n","# scores.loc[row, (\"Top-3 Acc\", \"value\")] = round(np.mean(top_3), 3)\n","# scores.loc[row, (\"Top-3 Acc\", \"std\")] = round(np.std(top_3), 3)\n","# scores.loc[row, (\"Macro F-Measure\", \"value\")] = round(np.mean(f1_macro), 3)\n","# scores.loc[row, (\"Macro F-Measure\", \"std\")] = round(np.std(f1_macro), 3)\n","# scores.loc[row, (\"Weighted F-Measure\", \"value\")] = round(np.mean(f1_weighted), 3)\n","# scores.loc[row, (\"Weighted F-Measure\", \"std\")] = round(np.std(f1_weighted), 3)\n","\n","# # with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"wb\") as outfile:\n","# #   pickle.dump(scores, outfile)\n","# #   outfile.close()\n","\n","\n","# scores"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vTXZqCE2KLa0","colab_type":"text"},"source":["### Punctual evaluation"]},{"cell_type":"code","metadata":{"id":"9IojprE-Vb5b","colab_type":"code","colab":{}},"source":["#----- if saved, load\n","# with np.load(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/d2v_stemm_concat.npz\") as infile:\n","#   train_vect = infile['X']\n","#   y = infile['y']\n","#   infile.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8RA1ZIZVC8u","colab_type":"code","colab":{}},"source":["#----- train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(train_vect, y, test_size = .2, random_state = 42, stratify = y)\n","X_train.shape, X_test.shape, y_train.shape, y_test.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4HC9sprar4rk","colab_type":"code","colab":{}},"source":["rf = RandomForestClassifier(n_jobs=-1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVIEXPL8ul_r","colab_type":"code","colab":{}},"source":["rf.fit(X_train, y_train)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qFRcTVRwHa4","colab_type":"code","colab":{}},"source":["y_pred = rf.predict(X_test)\n","\n","f1_macro = f1_score(y_test, y_pred, average = 'macro')\n","f1_weighted = f1_score(y_test, y_pred, average = 'weighted')\n","acc = accuracy_score(y_test, y_pred)\n","\n","acc, f1_macro, f1_weighted\n","\n","# # ['W2V Tf-idf', 'W2V mean', 'Doc2Vec']\n","# model = 'Doc2Vec (concat)'\n","# # ['Stemming', 'Stemming+Badwords', 'Lemmatization', 'Lemmatization+Badwords']\n","# representation = 'Stemming'\n","\n","# time.sleep(2)\n","# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/evaluations.pckl\", \"rb\") as infile:\n","#   evaluations = pickle.load(infile)\n","#   infile.close()\n","\n","# time.sleep(2)\n","# evaluations.loc[model, (representation, 'Acc')] = round(acc, 3)\n","# evaluations.loc[model, (representation, 'Macro F-Measure')] = round(f1_macro, 3)\n","# evaluations.loc[model, (representation, 'Weighted F-Measure')] = round(f1_weighted, 3)\n","\n","# time.sleep(2)\n","# #with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/evaluations.pckl\", \"wb\") as outfile:\n","#  # pickle.dump(evaluations, outfile)\n","#   #outfile.close()\n","\n","# evaluations"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UuYWzZaYh7Q","colab_type":"text"},"source":["## Dump evaluations & scores into Excel fashion"]},{"cell_type":"code","metadata":{"id":"5aEpixfnVgzR","colab_type":"code","colab":{}},"source":["# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/evaluations.pckl\", \"rb\") as infile:\n","#   evaluations = pickle.load(infile)\n","#   infile.close()\n","\n","# evaluations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Xi3FCwtVjlg","colab_type":"code","colab":{}},"source":["#evaluations.to_excel(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/evaluations.xlsx\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkrL0RFMV8Ci","colab_type":"code","colab":{}},"source":["# with open(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.pckl\", \"rb\") as infile:\n","#   scores = pickle.load(infile)\n","#   infile.close()\n","\n","# scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgVq4isTWAx4","colab_type":"code","colab":{}},"source":["#scores.to_excel(\"/content/drive/My Drive/Colab Notebooks/TM&S/text-representations&evaluations/scores.xlsx\")"],"execution_count":0,"outputs":[]}]}